深度学习最近被用来解决图像处理任务。特别的，深度学习展现的结果比传统方法更优，如计算随机变量的逐点估计器。本章，我们回顾了深度学习方法来求解线性逆问题。特别地，本节深度神经网络的损失函数的选择让我们来求解贝叶斯point-wise estimators. 3.2节，回顾不同的核心结构，对深度神经网络普遍的结构。3.3节，我们cover一些深度结构背后的理论。最后在3.4节，我们回顾一些深度神经网络里的经验细节。

我们考虑这样的逆问题恢复模型：
$m=Af+e$
$e--N(0,\Sigma_\alpha)$
该逆问题可以被用来模拟单像素成像的过程。当全贝叶斯inversion不可行，$f$的估计器可以通过条件概率$E(F|m)$来得到。
 直接的重构方法使用深度学习来提供一个映射关系来估计条件概率。
 $G(m)=E(f|m)$

前向深度神经网络是一个自然的选择，由于普遍的估计理论 认为它可以估计任何Borel-measurable函数从一个有限维空间到另一个空间。替换期望为它的经验empirical counter part被大数定理证明。根据


深度学习的基础结构

感知机

人工神经网络

卷积层
卷积神经网络展现了出色的表现当求解图像逆问题，如去躁，超分辨，解卷积和许多生物成像逆问题，如计算层析和加速核磁共振成像。这些是很多神经网络的基础框架。卷积层背后的理论和framelets的理论相关，凭此卷积层是一个线性空间的线性变换，使得求解逆问题更加容易。
实现卷积层的两个主要想法是使用局部感受野，使用共享的weights和bias在不同的感受野上
局部感受野：对于全连接层，输入和隐藏状态被表示为神经元的向量被展现在直线上。在卷积网络，将神经元表示为方形矩阵更有优势。如图3表示的，输入被连接到隐藏神经元上，但是不同于全连接层，每个神经元仅连接到层里的一小块的部分神经元中（被称为感受野）。局部感受野沿着整个图像移动来产生隐藏层。
共享weights和bias：一层里的每个神经元连接到。检测到相同的特征。
因此，卷积神经网络的隐藏层被称为特征图。
	卷积层可以产生很多特征图通过使用不同的卷积核。为了简化表示，卷积层的隐藏states通常被表示为 每一层是不同的特征图。 我们表示隐藏层

池化层
在卷积层之外，卷积神经网络也含有池化层，它们的目的是简化包含在特征图中的信息，通过减小特征图的维度来实现。
池化层因此输出一个降维的特征图，每一个元素从输入特征图的一块区域来计算。最常使用的是最大池化层。最大池化层由两个参数来定义：kernel大小和stride。kernel size决定了特征图中的区域。stride决定了我们进行下一次最大池化时的步长。这些层让我们来模仿多尺度分析：图像通过神经网络在不同的尺度上进行分析，这和小波分析的理论很类似。

U-net
在很多全连接层，卷积层和池化层的很多潜在组合中，Unet结构可能是最普遍的.
最初由Ronneberger来引入实现MRI的分割，它已经成功地被用来解决一些逆问题。